{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7abace-ddad-4316-8f03-dff9d99dfaa8",
   "metadata": {},
   "source": [
    "# 1.0 Source Data Exploration and Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e26b6-3fbe-4c48-8f60-1222d92812e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Exploring Raw Data\n",
    "\n",
    "Let's first explore what the raw data look like so we can asses the change needed in the pipeline to dynamically build schema objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53dd24d-0194-40e2-8105-cfe36bd1ca86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZUREAD_2022-08-30T00_17_05.561Z.json  AZUREAD_2022-08-31T00_21_46.153Z.json\n",
      "AZUREAD_2022-08-30T03_14_27.626Z.json  AZUREAD_2022-08-31T03_08_27.951Z.json\n",
      "AZUREAD_2022-08-30T06_15_21.422Z.json  AZUREAD_2022-08-31T06_20_09.178Z.json\n",
      "AZUREAD_2022-08-30T09_21_58.312Z.json  AZUREAD_2022-08-31T09_01_27.089Z.json\n",
      "AZUREAD_2022-08-30T12_05_53.775Z.json  AZUREAD_2022-08-31T12_02_02.230Z.json\n",
      "AZUREAD_2022-08-30T15_05_34.679Z.json  AZUREAD_2022-08-31T15_03_06.756Z.json\n",
      "AZUREAD_2022-08-30T18_39_54.214Z.json  AZUREAD_2022-08-31T18_03_06.102Z.json\n",
      "AZUREAD_2022-08-30T21_01_48.448Z.json  AZUREAD_2022-08-31T21_13_44.759Z.json\n"
     ]
    }
   ],
   "source": [
    "! ls /workspace/examples/data/dfp/azure-inference-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa944cce-13a1-4fe9-95c0-47613c09f843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 13 rows and inference data has 11 rows.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"../../../data/dfp/azure-training-data/AZUREAD_2022-08-01T00_03_56.207Z.json\") as f:\n",
    "    training_data_sample = json.load(f)\n",
    "    \n",
    "with open(\"../../../data/dfp/azure-inference-data/AZUREAD_2022-08-30T00_17_05.561Z.json\") as f:\n",
    "    inference_data_sample = json.load(f)\n",
    "    \n",
    "print(f\"Training data has {len(training_data_sample)} rows and inference data has {len(inference_data_sample)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31dc4bb7-80e6-4c38-85a7-179ef62571b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>resourceId</th>\n",
       "      <th>operationName</th>\n",
       "      <th>operationVersion</th>\n",
       "      <th>category</th>\n",
       "      <th>tenantId</th>\n",
       "      <th>resultType</th>\n",
       "      <th>resultSignature</th>\n",
       "      <th>resultDescription</th>\n",
       "      <th>durationMs</th>\n",
       "      <th>callerIpAddress</th>\n",
       "      <th>correlationId</th>\n",
       "      <th>identity</th>\n",
       "      <th>Level</th>\n",
       "      <th>location</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-01T00:03:56.207532Z</td>\n",
       "      <td>/tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...</td>\n",
       "      <td>Sign-in activity</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NonInteractiveUserSignInLogs</td>\n",
       "      <td>d3e5a967-5657-4a42-afcc-6106b6c3c299</td>\n",
       "      <td>50158</td>\n",
       "      <td>None</td>\n",
       "      <td>External security challenge was not satisfied.</td>\n",
       "      <td>0</td>\n",
       "      <td>44.22.19.201</td>\n",
       "      <td>84ca338d-f4ff-4f34-9f2a-5a6e23f78c0b</td>\n",
       "      <td>Thomas Price</td>\n",
       "      <td>4</td>\n",
       "      <td>XN</td>\n",
       "      <td>{'id': 'df70b726-7756-4baa-9a7d-5ac965198e00',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-01T00:19:37.909827Z</td>\n",
       "      <td>/tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...</td>\n",
       "      <td>Sign-in activity</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SignInLogs</td>\n",
       "      <td>d3e5a967-5657-4a42-afcc-6106b6c3c299</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>99.116.100.205</td>\n",
       "      <td>7641103c-1db3-4e14-9ebc-6a9555ba02b2</td>\n",
       "      <td>Aaron Cole</td>\n",
       "      <td>4</td>\n",
       "      <td>XD</td>\n",
       "      <td>{'id': 'c98bb980-53fe-43a8-afd2-72b917706b00',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-01T00:25:38.530749Z</td>\n",
       "      <td>/tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...</td>\n",
       "      <td>Sign-in activity</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NonInteractiveUserSignInLogs</td>\n",
       "      <td>d3e5a967-5657-4a42-afcc-6106b6c3c299</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>86.154.193.190</td>\n",
       "      <td>ef72b144-8295-493d-8231-c12e755a74d8</td>\n",
       "      <td>Kristen Howell</td>\n",
       "      <td>4</td>\n",
       "      <td>XR</td>\n",
       "      <td>{'id': '4ef53074-987d-44ae-a8dd-b6e418929900',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-01T00:37:00.149031Z</td>\n",
       "      <td>/tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...</td>\n",
       "      <td>Sign-in activity</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NonInteractiveUserSignInLogs</td>\n",
       "      <td>d3e5a967-5657-4a42-afcc-6106b6c3c299</td>\n",
       "      <td>50158</td>\n",
       "      <td>None</td>\n",
       "      <td>External security challenge was not satisfied.</td>\n",
       "      <td>0</td>\n",
       "      <td>42.62.103.34</td>\n",
       "      <td>9901c16e-f768-4891-9b92-f1ab68223893</td>\n",
       "      <td>Joseph Taylor</td>\n",
       "      <td>4</td>\n",
       "      <td>XF</td>\n",
       "      <td>{'id': '7f19788f-2e61-49ad-9601-4fe6e5b87200',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-01T00:44:19.056251Z</td>\n",
       "      <td>/tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...</td>\n",
       "      <td>Sign-in activity</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NonInteractiveUserSignInLogs</td>\n",
       "      <td>d3e5a967-5657-4a42-afcc-6106b6c3c299</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>42.62.103.34</td>\n",
       "      <td>c25d344f-ea74-4470-94bb-ea652c630dd3</td>\n",
       "      <td>Joseph Taylor</td>\n",
       "      <td>4</td>\n",
       "      <td>XF</td>\n",
       "      <td>{'id': '20f677f0-ddf9-46df-bf50-9e296caa9100',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          time  \\\n",
       "0  2022-08-01T00:03:56.207532Z   \n",
       "1  2022-08-01T00:19:37.909827Z   \n",
       "2  2022-08-01T00:25:38.530749Z   \n",
       "3  2022-08-01T00:37:00.149031Z   \n",
       "4  2022-08-01T00:44:19.056251Z   \n",
       "\n",
       "                                          resourceId     operationName  \\\n",
       "0  /tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...  Sign-in activity   \n",
       "1  /tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...  Sign-in activity   \n",
       "2  /tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...  Sign-in activity   \n",
       "3  /tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...  Sign-in activity   \n",
       "4  /tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/...  Sign-in activity   \n",
       "\n",
       "  operationVersion                      category  \\\n",
       "0              1.0  NonInteractiveUserSignInLogs   \n",
       "1              1.0                    SignInLogs   \n",
       "2              1.0  NonInteractiveUserSignInLogs   \n",
       "3              1.0  NonInteractiveUserSignInLogs   \n",
       "4              1.0  NonInteractiveUserSignInLogs   \n",
       "\n",
       "                               tenantId resultType resultSignature  \\\n",
       "0  d3e5a967-5657-4a42-afcc-6106b6c3c299      50158            None   \n",
       "1  d3e5a967-5657-4a42-afcc-6106b6c3c299          0            None   \n",
       "2  d3e5a967-5657-4a42-afcc-6106b6c3c299          0            None   \n",
       "3  d3e5a967-5657-4a42-afcc-6106b6c3c299      50158            None   \n",
       "4  d3e5a967-5657-4a42-afcc-6106b6c3c299          0            None   \n",
       "\n",
       "                                resultDescription  durationMs callerIpAddress  \\\n",
       "0  External security challenge was not satisfied.           0    44.22.19.201   \n",
       "1                                            None           0  99.116.100.205   \n",
       "2                                            None           0  86.154.193.190   \n",
       "3  External security challenge was not satisfied.           0    42.62.103.34   \n",
       "4                                            None           0    42.62.103.34   \n",
       "\n",
       "                          correlationId        identity  Level location  \\\n",
       "0  84ca338d-f4ff-4f34-9f2a-5a6e23f78c0b    Thomas Price      4       XN   \n",
       "1  7641103c-1db3-4e14-9ebc-6a9555ba02b2      Aaron Cole      4       XD   \n",
       "2  ef72b144-8295-493d-8231-c12e755a74d8  Kristen Howell      4       XR   \n",
       "3  9901c16e-f768-4891-9b92-f1ab68223893   Joseph Taylor      4       XF   \n",
       "4  c25d344f-ea74-4470-94bb-ea652c630dd3   Joseph Taylor      4       XF   \n",
       "\n",
       "                                          properties  \n",
       "0  {'id': 'df70b726-7756-4baa-9a7d-5ac965198e00',...  \n",
       "1  {'id': 'c98bb980-53fe-43a8-afd2-72b917706b00',...  \n",
       "2  {'id': '4ef53074-987d-44ae-a8dd-b6e418929900',...  \n",
       "3  {'id': '7f19788f-2e61-49ad-9601-4fe6e5b87200',...  \n",
       "4  {'id': '20f677f0-ddf9-46df-bf50-9e296caa9100',...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(training_data_sample).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72629a2f-38db-4787-a420-4a53711b864d",
   "metadata": {},
   "source": [
    "We see a combination of mixed data types in the dataframe. Our Morpheus pipeline will require us to cast the input dataframe into uniform types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2b4a8-3166-4640-9fc6-3b6ed5ec3fc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 An Overview of DataFrame Schemas\n",
    "\n",
    "Recall that Morpheus requires us to create ```DataFrameInputSchema``` objects to pass to the ```DFPFileToDataFrameStage``` and ```DFPPreprocessingStage``` so it knows how and where to manipulate objects from the JSON as CuDF dataframes are processed. \n",
    "\n",
    "```DataFrameInputSchema``` objects are created from lists of ```ColumnInfo``` objects. For example, Here's what we're aiming to get to dynamically in the pipeline for the source stage:\n",
    "\n",
    "```python\n",
    "source_column_info = [\n",
    "            DateTimeColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=\"time\"),\n",
    "            RenameColumn(name=config.ae.userid_column_name, dtype=str, input_name=\"properties.userPrincipalName\"),\n",
    "            RenameColumn(name=\"appDisplayName\", dtype=str, input_name=\"properties.appDisplayName\"),\n",
    "            ColumnInfo(name=\"category\", dtype=str),\n",
    "            RenameColumn(name=\"clientAppUsed\", dtype=str, input_name=\"properties.clientAppUsed\"),\n",
    "            RenameColumn(name=\"deviceDetailbrowser\", dtype=str, input_name=\"properties.deviceDetail.browser\"),\n",
    "            RenameColumn(name=\"deviceDetaildisplayName\", dtype=str, input_name=\"properties.deviceDetail.displayName\"),\n",
    "            RenameColumn(name=\"deviceDetailoperatingSystem\",\n",
    "                         dtype=str,\n",
    "                         input_name=\"properties.deviceDetail.operatingSystem\"),\n",
    "            StringCatColumn(name=\"location\",\n",
    "                            dtype=str,\n",
    "                            input_columns=[\n",
    "                                \"properties.location.city\",\n",
    "                                \"properties.location.countryOrRegion\",\n",
    "                            ],\n",
    "                            sep=\", \"),\n",
    "            RenameColumn(name=\"statusfailureReason\", dtype=str, input_name=\"properties.status.failureReason\"),\n",
    "        ]\n",
    "\n",
    "```\n",
    "\n",
    "You'll notice a few things here. There are three primary types of Column types used in this schema definition:\n",
    "1. `ColumnInfo`: This is a 'fundamental' column type on Morpheus, storing a column name and data type for a single column in a dataframe. ColumnInfo objects are defined as python ```dataclasses``` and contain a host of utility functions to transform data. Refer to source at ```morpheus/utils/column_info.py``` for more details. \n",
    "2. `StringCatColumn`: Subclass of `ColumnInfo`, concatenates values from multiple columns into a new string column separated by `sep`. This is useful when we want to create a single derived categorical feature from multiple columns in a dataset. \n",
    "3. `RenameColumn`: This class derives from the ```ColumnInfo``` class to also allow for dynamic renaming of the column in the pipeline. Notice above that some columns are renamed to align with the original AE feature values we defined in the Config above. \n",
    "4. `DateTimeColumn`: This is a subclass of ```RenameColumn```, specific to casting UTC localized datetime values. When incoming values contain a\n",
    "    time-zone offset string the values are converted to UTC, while values without a time-zone are assumed to be UTC.\n",
    "    \n",
    "Once the list is created, ```DataFrameInputSchema``` objects are created as follows:\n",
    "\n",
    "```python\n",
    "source_schema = DataFrameInputSchema(json_columns=[\"properties\"], column_info=source_column_info)\n",
    "```\n",
    "\n",
    "One more thing to note above is that Morpheus also allows you to ***preserve nested JSON objects*** present in columns in the output dataframe. Here, we declare that we want to preserve the `properties` column in the output dataframe. This will flatten the JSON content of the column to individual columns, where the heriarchy of the element is reprented by '.' separated keys in the column. You can see the usage of this flattenning above in almost all lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d076611-85ce-4e34-82a4-53959f60178d",
   "metadata": {},
   "source": [
    "## 1.2 Creating Schemas from JSON Definitions\n",
    "\n",
    "Here, we explore if it is possible to dynamically generate sceham definitions defined above if the user provides a JSON schema definitions which can be interpreted at runtime. As an alpha version, we'll focus on five python datatypes which will be supported: `string`, `float`, `int`, `bool`, and `datetime`. \n",
    "\n",
    "We'll also support the following `ColumnInfo` types: `ColumnInfo`, `RenameColumn`,`BoolColumn`,`DateTimeColumn`,`StringJoinColumn`,`StringCatColumn`,`IncrementColumn`,and `DistinctIncrementColumn`. \n",
    "\n",
    "The user will provide a path to a schema_defintion.json file containing information about the schema definitoin they want to build, and the pipeline will dynamically verify the consistency of the definition and construct the schema. Let's look at an example of a JSON schema definition that would create a schema as defined in section 2.1.\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"JSON_COLUMNS\" : [\"properties\"],\n",
    "    \"PRESERVE_COLUMNS\": [],\n",
    "    \"SCHEMA_COLUMNS\" : [\n",
    "        {\n",
    "            \"type\" : \"DateTimeColumn\",\n",
    "            \"name\" : \"timestamp\",\n",
    "            \"dtype\": \"datetime\",\n",
    "            \"data_column\" : \"time\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"RenameColumn\",\n",
    "            \"name\" : \"username\",\n",
    "            \"dtype\": \"string\",\n",
    "            \"data_column\" : \"properties.userPrincipalName\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"RenameColumn\",\n",
    "            \"name\" : \"appDisplayName\",\n",
    "            \"dtype\": \"string\",\n",
    "            \"data_column\" : \"properties.appDisplayName\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"ColumnInfo\",\n",
    "            \"data_column\" : \"category\",\n",
    "            \"dtype\" : \"string\"\n",
    "        },\n",
    "        ... //Other Definitions\n",
    "        {\n",
    "            \"type\" : \"StringCatColumn\",\n",
    "            \"name\" : \"location\",\n",
    "            \"dtype\": \"string\",\n",
    "            \"data_columns\" : [\"properties.location.city\", \"properties.location.countryOrRegion\"]\n",
    "            \"sep\" : \", \"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Most of the structure above is fairly intuitive. `JSON_COLUMNS` indicated which parts of the dataframe contains a JSON type that needs to be flattened. Each of the `SCHEMA_COLUMN` objects will become one `ColumnInfo` definition in our schema list. One notable change is that the `input_name` argument in our code is changed to `data_column` in the JSON to increase clarity for the end user. \n",
    "\n",
    "Operationalizing something like this requires us to create a a class that can read the JSON file, validate its structure, and create the schema. Let's look at how we can use one below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f212e15b-8dec-46c2-9a51-7c433127a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \n",
      "    \"JSON_COLUMNS\" : [\"properties\"],\n",
      "    \"SCHEMA_COLUMNS\" : [\n",
      "        {\n",
      "            \"type\" : \"DateTimeColumn\",\n",
      "            \"dtype\" : \"datetime\",\n",
      "            \"name\" : \"timestamp\",\n",
      "            \"data_column\" : \"time\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"RenameColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"username\",\n",
      "            \"data_column\" : \"properties.userPrincipalName\"        \n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"RenameColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"appDisplayName\",\n",
      "            \"data_column\" : \"properties.appDisplayName\"        \n",
      "        }, \n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"category\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"RenameColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"clientAppUsed\",\n",
      "            \"data_column\" : \"properties.clientAppUsed\"       \n",
      "        }, \n",
      "        {\n",
      "            \"type\" : \"RenameColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"deviceDetailbrowser\",\n",
      "            \"data_column\" : \"properties.deviceDetail.browser\"        \n",
      "        }, \n",
      "        {\n",
      "            \"type\" : \"RenameColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"deviceDetaildisplayName\",\n",
      "            \"data_column\" : \"properties.deviceDetail.displayName\"        \n",
      "        }, \n",
      "        {\n",
      "            \"type\" : \"RenameColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"deviceDetailoperatingSystem\",\n",
      "            \"data_column\" : \"properties.deviceDetail.operatingSystem\"     \n",
      "        }, \n",
      "        {\n",
      "            \"type\" : \"StringCatColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"location\",\n",
      "            \"data_columns\" : [\n",
      "                                \"properties.location.city\",\n",
      "                                \"properties.location.countryOrRegion\"\n",
      "                            ],\n",
      "            \"sep\" : \", \"\n",
      "        },  \n",
      "        {\n",
      "            \"type\" : \"RenameColumn\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"name\" : \"statusfailureReason\",\n",
      "            \"data_column\" : \"properties.status.failureReason\"     \n",
      "        } \n",
      "    ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "# First, we look at an example input data schema\n",
    "! cat /workspace/models/data/dfp_azure_input_schema_example.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06012ba6-0ba3-4fb2-8145-ba3167e9287c",
   "metadata": {},
   "source": [
    "To an end user, specifying data schemas like the above is more intuitive than writing python to specify the schema. Our provided parser can also validate data types provided in such a JSON to ensure correctness. Let's see how we could use this with a parser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff958f4-9cba-419a-a3b7-afb93aa37f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameInputSchema(json_columns=['properties'],\n",
      "                     column_info=[DateTimeColumn(name='timestamp',\n",
      "                                                 dtype='datetime64[ns]',\n",
      "                                                 input_name='time'),\n",
      "                                  RenameColumn(name='username',\n",
      "                                               dtype='string',\n",
      "                                               input_name='properties.userPrincipalName'),\n",
      "                                  RenameColumn(name='appDisplayName',\n",
      "                                               dtype='string',\n",
      "                                               input_name='properties.appDisplayName'),\n",
      "                                  ColumnInfo(name='category', dtype='str'),\n",
      "                                  RenameColumn(name='clientAppUsed',\n",
      "                                               dtype='string',\n",
      "                                               input_name='properties.clientAppUsed'),\n",
      "                                  RenameColumn(name='deviceDetailbrowser',\n",
      "                                               dtype='string',\n",
      "                                               input_name='properties.deviceDetail.browser'),\n",
      "                                  RenameColumn(name='deviceDetaildisplayName',\n",
      "                                               dtype='string',\n",
      "                                               input_name='properties.deviceDetail.displayName'),\n",
      "                                  RenameColumn(name='deviceDetailoperatingSystem',\n",
      "                                               dtype='string',\n",
      "                                               input_name='properties.deviceDetail.operatingSystem'),\n",
      "                                  StringCatColumn(name='location',\n",
      "                                                  dtype='string',\n",
      "                                                  input_columns=['properties.location.city',\n",
      "                                                                 'properties.location.countryOrRegion'],\n",
      "                                                  sep=', '),\n",
      "                                  RenameColumn(name='statusfailureReason',\n",
      "                                               dtype='string',\n",
      "                                               input_name='properties.status.failureReason')],\n",
      "                     preserve_columns=None,\n",
      "                     row_filter=None)\n"
     ]
    }
   ],
   "source": [
    "from morpheus.utils.json_schema_builder import JSONSchemaBuilder\n",
    "\n",
    "schema_builder = JSONSchemaBuilder()\n",
    "schema = schema_builder.build_schema(\"/workspace/models/data/dfp_azure_input_schema_example.json\")\n",
    "\n",
    "pprint.pprint(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e36ce-4ab7-434d-9a72-30060584383f",
   "metadata": {},
   "source": [
    "Voila! A schema was constructed dynamically from the JSON definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8076e-c698-4971-b99e-b719b109a820",
   "metadata": {},
   "source": [
    "# 2.0 Putting it Together in a Pipeline\n",
    "\n",
    "Let's use the previous schema definition in a simple DFP Azure training pipeline to demonstrate it's capacity to simplify our code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c119dc-5caf-4d43-a4e6-48f217e050f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Imports and Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41268be-81b0-4ec1-826e-04793f4b5830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../../../../morpheus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ad2ce0-9003-4358-8f1c-c254bce5d988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import logging\n",
    "import os\n",
    "import mlflow\n",
    "import typing\n",
    "from datetime import datetime\n",
    "\n",
    "from dfp.stages.dfp_file_batcher_stage import DFPFileBatcherStage\n",
    "from dfp.stages.dfp_file_to_df import DFPFileToDataFrameStage\n",
    "from dfp.stages.dfp_mlflow_model_writer import DFPMLFlowModelWriterStage\n",
    "from dfp.stages.dfp_preprocessing_stage import DFPPreprocessingStage\n",
    "from dfp.stages.dfp_rolling_window_stage import DFPRollingWindowStage\n",
    "from dfp.stages.dfp_split_users_stage import DFPSplitUsersStage\n",
    "from dfp.stages.dfp_training import DFPTraining\n",
    "from dfp.stages.multi_file_source import MultiFileSource\n",
    "from dfp.utils.regex_utils import iso_date_regex\n",
    "\n",
    "from morpheus.common import FileTypes\n",
    "from morpheus.cli.utils import get_log_levels\n",
    "from morpheus.cli.utils import get_package_relative_file\n",
    "from morpheus.cli.utils import load_labels_file\n",
    "from morpheus.cli.utils import parse_log_level\n",
    "from morpheus.config import Config\n",
    "from morpheus.config import ConfigAutoEncoder\n",
    "from morpheus.config import CppConfig\n",
    "from morpheus.pipeline import LinearPipeline\n",
    "from morpheus.utils.column_info import ColumnInfo\n",
    "from morpheus.utils.column_info import DataFrameInputSchema\n",
    "from morpheus.utils.column_info import DateTimeColumn\n",
    "from morpheus.utils.column_info import DistinctIncrementColumn\n",
    "from morpheus.utils.column_info import IncrementColumn\n",
    "from morpheus.utils.column_info import RenameColumn\n",
    "from morpheus.utils.column_info import StringCatColumn\n",
    "from morpheus.utils.file_utils import date_extractor\n",
    "from morpheus.utils.logger import configure_logging\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2be73ca1-9a4f-4915-aa1e-f31f3d6c3b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global options\n",
    "train_users = \"all\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_users: typing.List[str] = []\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"/workspace/.cache/dfp\"\n",
    "\n",
    "# Input files to read from\n",
    "input_files = [\n",
    "    \"../../../../data/dfp/azure-training-data/AZUREAD_2022-08-0*.json\"\n",
    "]\n",
    "\n",
    "# The format to use for models\n",
    "model_name_formatter = \"DFP-azure-{user_id}\"\n",
    "\n",
    "# The format to use for experiment names\n",
    "experiment_name_formatter = \"dfp/azure/training/{reg_model_name}\"\n",
    "\n",
    "# === Derived Options ===\n",
    "# To include the generic, we must be training all or generic\n",
    "include_generic = train_users == \"all\" or train_users == \"generic\"\n",
    "\n",
    "# To include individual, we must be either training or inferring\n",
    "include_individual = train_users != \"generic\"\n",
    "\n",
    "# None indicates we arent training anything\n",
    "is_training = train_users != \"none\"\n",
    "\n",
    "# Tracking URI\n",
    "tracking_uri = \"http://mlflow:5000\"\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "140fe6ad-6c28-45cb-ac8f-55989266502d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable the Morpheus logger\n",
    "config = Config()\n",
    "\n",
    "CppConfig.set_should_use_cpp(False)\n",
    "\n",
    "config.num_threads = os.cpu_count()\n",
    "\n",
    "config.ae = ConfigAutoEncoder()\n",
    "\n",
    "config.ae.feature_columns = [\n",
    "    \"appDisplayName\", \"clientAppUsed\", \"deviceDetailbrowser\", \"deviceDetaildisplayName\", \"deviceDetailoperatingSystem\", \"statusfailureReason\", \"appincrement\", \"locincrement\", \"logcount\", \n",
    "]\n",
    "config.ae.userid_column_name = \"username\"\n",
    "config.ae.timestamp_column_name = \"timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4499a03-18af-45a9-8da9-dddf07d921d5",
   "metadata": {},
   "source": [
    "## 2.2 Schema Definition (here's where we use the JSONSchemaBuilder)\n",
    "First, we'll define a schema for the input of the raw data using the `JSONSchemaBuilder`. We'll also define a data schema for the post-processed data using older manual definitions without a schema builder to compare the difference in complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39efd9b0-778c-41d1-89dc-4a7f53d3a849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the JSONSchemaBuilder\n",
    "from dfp.utils.json_schema_builder import JSONSchemaBuilder\n",
    "\n",
    "schema_builder = JSONSchemaBuilder()\n",
    "source_schema = schema_builder.build_schema(\"/workspace/models/data/dfp_azure_input_schema_example.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c63283-19e6-4c46-8cea-eae9925883c6",
   "metadata": {},
   "source": [
    "That's ***all we had to do*** to build an input schema. Let's compare that to how we define a preprocessing schema without a builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3c1b8ea-e955-422e-a802-5f87447a55de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameInputSchema(json_columns=[],\n",
      "                     column_info=[ColumnInfo(name='timestamp',\n",
      "                                             dtype='datetime64[ns]'),\n",
      "                                  ColumnInfo(name='username', dtype='str'),\n",
      "                                  ColumnInfo(name='appDisplayName',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='clientAppUsed', dtype='str'),\n",
      "                                  ColumnInfo(name='deviceDetailbrowser',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='deviceDetaildisplayName',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='deviceDetailoperatingSystem',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='statusfailureReason',\n",
      "                                             dtype='str'),\n",
      "                                  IncrementColumn(name='logcount',\n",
      "                                                  dtype='int',\n",
      "                                                  input_name='timestamp',\n",
      "                                                  groupby_column='username',\n",
      "                                                  period='D'),\n",
      "                                  DistinctIncrementColumn(name='locincrement',\n",
      "                                                          dtype='int',\n",
      "                                                          input_name='location',\n",
      "                                                          groupby_column='username',\n",
      "                                                          period='D',\n",
      "                                                          timestamp_column='timestamp'),\n",
      "                                  DistinctIncrementColumn(name='appincrement',\n",
      "                                                          dtype='int',\n",
      "                                                          input_name='appDisplayName',\n",
      "                                                          groupby_column='username',\n",
      "                                                          period='D',\n",
      "                                                          timestamp_column='timestamp')],\n",
      "                     preserve_columns=re.compile('(_batch_id)'),\n",
      "                     row_filter=None)\n"
     ]
    }
   ],
   "source": [
    "preprocess_column_info = [\n",
    "    ColumnInfo(name=config.ae.timestamp_column_name, dtype=datetime),\n",
    "    ColumnInfo(name=config.ae.userid_column_name, dtype=str),\n",
    "    ColumnInfo(name=\"appDisplayName\", dtype=str),\n",
    "    ColumnInfo(name=\"clientAppUsed\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetailbrowser\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetaildisplayName\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetailoperatingSystem\", dtype=str),\n",
    "    ColumnInfo(name=\"statusfailureReason\", dtype=str),\n",
    "\n",
    "    # Derived columns\n",
    "    IncrementColumn(name=\"logcount\",\n",
    "                    dtype=int,\n",
    "                    input_name=config.ae.timestamp_column_name,\n",
    "                    groupby_column=config.ae.userid_column_name),\n",
    "    DistinctIncrementColumn(name=\"locincrement\",\n",
    "                            dtype=int,\n",
    "                            input_name=\"location\",\n",
    "                            groupby_column=config.ae.userid_column_name,\n",
    "                            timestamp_column=config.ae.timestamp_column_name),\n",
    "    DistinctIncrementColumn(name=\"appincrement\",\n",
    "                            dtype=int,\n",
    "                            input_name=\"appDisplayName\",\n",
    "                            groupby_column=config.ae.userid_column_name,\n",
    "                            timestamp_column=config.ae.timestamp_column_name)\n",
    "]\n",
    "\n",
    "preprocess_schema = DataFrameInputSchema(column_info=preprocess_column_info, preserve_columns=[\"_batch_id\"])\n",
    "\n",
    "pprint.pprint(preprocess_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc14f0-68ce-4ad1-968c-0aaa7dc95ed0",
   "metadata": {},
   "source": [
    "Using a schema builder clearly simplifies this process from a code prespective but also because an end-user does not have to understand the low-level implementation of schemas, but can implement it in a JSON object which is dynamically type-checked. **What if we were to re-implement the postprocess schema using our JSONSchemaBuilder?** Let's see how below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9814027-42fe-40b3-84f7-929691befdf3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"PRESERVE_COLUMNS\" : [\"_batch_id\"],\n",
      "    \"SCHEMA_COLUMNS\" : [\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"datetime\",\n",
      "            \"data_column\" : \"timestamp\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"username\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"appDisplayName\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"clientAppUsed\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"deviceDetailbrowser\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"deviceDetaildisplayName\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"deviceDetailoperatingSystem\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"ColumnInfo\",\n",
      "            \"dtype\" : \"string\",\n",
      "            \"data_column\" : \"statusfailureReason\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"IncrementColumn\",\n",
      "            \"dtype\" : \"int\",\n",
      "            \"name\" : \"logcount\",\n",
      "            \"data_column\" : \"timestamp\",\n",
      "            \"groupby_column\" : \"username\"         \n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"DistinctIncrementColumn\",\n",
      "            \"dtype\" : \"int\",\n",
      "            \"name\" : \"locincrement\",\n",
      "            \"data_column\" : \"location\",\n",
      "            \"groupby_column\" : \"username\",\n",
      "            \"timestamp_column\" : \"timestamp\"\n",
      "        },\n",
      "        {\n",
      "            \"type\" : \"DistinctIncrementColumn\",\n",
      "            \"dtype\" : \"int\",\n",
      "            \"name\" : \"appincrement\",\n",
      "            \"data_column\" : \"appDisplayName\",\n",
      "            \"groupby_column\" : \"username\",\n",
      "            \"timestamp_column\" : \"timestamp\"\n",
      "        }\n",
      "        \n",
      "    ]\n",
      "    \n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat dfp_azure_preprocess_schema_example.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9f26f67-607c-4046-8dd5-de03d149fc37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameInputSchema(json_columns=[],\n",
      "                     column_info=[ColumnInfo(name='timestamp',\n",
      "                                             dtype='datetime64[ns]'),\n",
      "                                  ColumnInfo(name='username', dtype='str'),\n",
      "                                  ColumnInfo(name='appDisplayName',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='clientAppUsed', dtype='str'),\n",
      "                                  ColumnInfo(name='deviceDetailbrowser',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='deviceDetaildisplayName',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='deviceDetailoperatingSystem',\n",
      "                                             dtype='str'),\n",
      "                                  ColumnInfo(name='statusfailureReason',\n",
      "                                             dtype='str'),\n",
      "                                  IncrementColumn(name='logcount',\n",
      "                                                  dtype='int',\n",
      "                                                  input_name='timestamp',\n",
      "                                                  groupby_column='username',\n",
      "                                                  period='D'),\n",
      "                                  DistinctIncrementColumn(name='locincrement',\n",
      "                                                          dtype='int',\n",
      "                                                          input_name='location',\n",
      "                                                          groupby_column='username',\n",
      "                                                          period='D',\n",
      "                                                          timestamp_column='timestamp'),\n",
      "                                  DistinctIncrementColumn(name='appincrement',\n",
      "                                                          dtype='int',\n",
      "                                                          input_name='appDisplayName',\n",
      "                                                          groupby_column='username',\n",
      "                                                          period='D',\n",
      "                                                          timestamp_column='timestamp')],\n",
      "                     preserve_columns=re.compile('(_batch_id)'),\n",
      "                     row_filter=None)\n"
     ]
    }
   ],
   "source": [
    "preprocess_schema = schema_builder.build_schema(\"/workspace/models/data/dfp_azure_preprocess_schema_example.json\")\n",
    "pprint.pprint(preprocess_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5ab8e-562e-45fd-abbc-fdb7e6fadb32",
   "metadata": {},
   "source": [
    "## 2.3 The Rest of the Pipeline\n",
    "\n",
    "Now, we'll run the rest of the pipeline to demonstrate how schema's fit into the larger puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "039f6f73-3cfa-4694-8cee-ba97d3927bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Pipeline Pre-build====\u001b[0m\n",
      "====Pre-Building Segment: linear_segment_0====\u001b[0m\n",
      "====Pre-Building Segment Complete!====\u001b[0m\n",
      "====Pipeline Pre-build Complete!====\u001b[0m\n",
      "====Registering Pipeline====\u001b[0m\n",
      "====Building Pipeline====\u001b[0m\n",
      "====Building Pipeline Complete!====\u001b[0m\n",
      "====Registering Pipeline Complete!====\u001b[0m\n",
      "====Starting Pipeline====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W20240425 21:03:30.542733  2618 logging.cpp:67] MRC logger already initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Pipeline Started====\u001b[0m\n",
      "====Building Segment: linear_segment_0====\u001b[0m\n",
      "Added source: <from-multi-file-24; MultiFileSource(filenames=['../../../data/dfp/azure-training-data/AZUREAD_2022-08-0*.json'], watch=False, watch_interval=1.0)>\n",
      "  └─> fsspec.OpenFiles\u001b[0m\n",
      "Added stage: <dfp-file-batcher-25; DFPFileBatcherStage(date_conversion_func=functools.partial(<function date_extractor at 0x7f7e741e5510>, filename_regex=re.compile('(?P<year>\\\\d{4})-(?P<month>\\\\d{1,2})-(?P<day>\\\\d{1,2})(?:T(?P<hour>\\\\d{1,2})(?::|_|\\\\.)(?P<minute>\\\\d{1,2})(?::|_|\\\\.)(?P<second>\\\\d{1,2})(?:\\\\.(?P<microsecond>\\\\d{0,6}))?)?(?P<zulu>Z)?')), period=D, sampling_rate_s=None, start_time=None, end_time=None, sampling=None)>\n",
      "  └─ fsspec.OpenFiles -> Tuple[fsspec.core.OpenFiles, int]\u001b[0m\n",
      "Added stage: <dfp-file-to-df-26; DFPFileToDataFrameStage(schema=DataFrameInputSchema(json_columns=['properties'], column_info=[DateTimeColumn(name='timestamp', dtype='datetime64[ns]', input_name='time'), RenameColumn(name='username', dtype='string', input_name='properties.userPrincipalName'), RenameColumn(name='appDisplayName', dtype='string', input_name='properties.appDisplayName'), ColumnInfo(name='category', dtype='str'), RenameColumn(name='clientAppUsed', dtype='string', input_name='properties.clientAppUsed'), RenameColumn(name='deviceDetailbrowser', dtype='string', input_name='properties.deviceDetail.browser'), RenameColumn(name='deviceDetaildisplayName', dtype='string', input_name='properties.deviceDetail.displayName'), RenameColumn(name='deviceDetailoperatingSystem', dtype='string', input_name='properties.deviceDetail.operatingSystem'), StringCatColumn(name='location', dtype='string', input_columns=['properties.location.city', 'properties.location.countryOrRegion'], sep=', '), RenameColumn(name='statusfailureReason', dtype='string', input_name='properties.status.failureReason')], preserve_columns=None, row_filter=None), filter_null=True, file_type=FileTypes.JSON, parser_kwargs={'lines': False, 'orient': 'records'}, cache_dir=/workspace/.cache/dfp)>\n",
      "  └─ Tuple[fsspec.core.OpenFiles, int] -> pandas.DataFrame\u001b[0m\n",
      "Added stage: <dfp-split-users-27; DFPSplitUsersStage(include_generic=True, include_individual=True, skip_users=[], only_users=None)>\n",
      "  └─ pandas.DataFrame -> dfp.DFPMessageMeta\u001b[0m\n",
      "Added stage: <dfp-rolling-window-28; DFPRollingWindowStage(min_history=300, min_increment=300, max_history=60d, cache_dir=/workspace/.cache/dfp)>\n",
      "  └─ dfp.DFPMessageMeta -> dfp.MultiDFPMessage\u001b[0m\n",
      "Added stage: <dfp-preproc-29; DFPPreprocessingStage(input_schema=DataFrameInputSchema(json_columns=[], column_info=[ColumnInfo(name='timestamp', dtype='datetime64[ns]'), ColumnInfo(name='username', dtype='str'), ColumnInfo(name='appDisplayName', dtype='str'), ColumnInfo(name='clientAppUsed', dtype='str'), ColumnInfo(name='deviceDetailbrowser', dtype='str'), ColumnInfo(name='deviceDetaildisplayName', dtype='str'), ColumnInfo(name='deviceDetailoperatingSystem', dtype='str'), ColumnInfo(name='statusfailureReason', dtype='str'), IncrementColumn(name='logcount', dtype='int', input_name='timestamp', groupby_column='username', period='D'), DistinctIncrementColumn(name='locincrement', dtype='int', input_name='location', groupby_column='username', period='D', timestamp_column='timestamp'), DistinctIncrementColumn(name='appincrement', dtype='int', input_name='appDisplayName', groupby_column='username', period='D', timestamp_column='timestamp')], preserve_columns=re.compile('(_batch_id)'), row_filter=None))>\n",
      "  └─ dfp.MultiDFPMessage -> dfp.MultiDFPMessage\u001b[0m\n",
      "Added stage: <dfp-training-30; DFPTraining(model_kwargs=None, epochs=30, validation_size=0.1)>\n",
      "  └─ dfp.MultiDFPMessage -> morpheus.MultiAEMessage\u001b[0m\n",
      "Added stage: <dfp-mlflow-model-writer-31; DFPMLFlowModelWriterStage(model_name_formatter=DFP-azure-{user_id}, experiment_name_formatter=dfp/azure/training/{reg_model_name}, databricks_permissions=None, timeout=1.0)>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MultiAEMessage\u001b[0m\n",
      "====Building Segment Complete!====\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 88, Cache: hit, Duration: 5.058765411376953 ms, Rate: 17395.548685078706 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 88 rows from 2022-08-01 00:03:56.207532 to 2022-08-01 23:54:11.248402. Output: 20 users, rows/user min: 1, max: 88, avg: 8.80. Duration: 5.53 ms\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 110, Cache: hit, Duration: 35.0339412689209 ms, Rate: 3139.8123081739177 rows/s\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 97, Cache: hit, Duration: 13.434410095214844 ms, Rate: 7220.264925108256 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 110 rows from 2022-08-02 00:03:57.781586 to 2022-08-02 23:58:42.803775. Output: 19 users, rows/user min: 1, max: 110, avg: 11.58. Duration: 12.13 ms\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 126, Cache: hit, Duration: 7.191658020019531 ms, Rate: 17520.299164567034 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 97 rows from 2022-08-03 00:10:42.770060 to 2022-08-03 23:23:43.932133. Output: 16 users, rows/user min: 1, max: 97, avg: 12.12. Duration: 7.98 ms\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 126 rows from 2022-08-04 00:47:51.564611 to 2022-08-04 23:50:26.072379. Output: 21 users, rows/user min: 1, max: 126, avg: 12.00. Duration: 14.01 ms\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 109, Cache: hit, Duration: 61.92493438720703 ms, Rate: 1760.1956478216007 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 109 rows from 2022-08-05 00:14:48.503160 to 2022-08-05 23:45:07.826898. Output: 16 users, rows/user min: 1, max: 109, avg: 13.62. Duration: 14.18 ms\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 107, Cache: hit, Duration: 32.65953063964844 ms, Rate: 3276.225894995036 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 107 rows from 2022-08-06 00:08:48.348649 to 2022-08-06 23:53:00.392382. Output: 17 users, rows/user min: 1, max: 107, avg: 12.59. Duration: 9.16 ms\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 107, Cache: hit, Duration: 19.647836685180664 ms, Rate: 5445.892171971508 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 107 rows from 2022-08-07 00:00:23.959795 to 2022-08-07 23:56:24.809043. Output: 17 users, rows/user min: 1, max: 107, avg: 12.59. Duration: 13.24 ms\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 119, Cache: hit, Duration: 35.42184829711914 ms, Rate: 3359.5084875816115 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 119 rows from 2022-08-08 00:16:13.439012 to 2022-08-08 23:58:43.815912. Output: 18 users, rows/user min: 1, max: 119, avg: 13.22. Duration: 13.55 ms\u001b[0m\n",
      "\u001b[2mS3 objects to DF complete. Rows: 102, Cache: hit, Duration: 54.68249320983887 ms, Rate: 1865.3136317063068 rows/s\u001b[0m\n",
      "\u001b[2mBatch split users complete. Input: 102 rows from 2022-08-09 00:23:17.790393 to 2022-08-09 23:59:49.626250. Output: 16 users, rows/user min: 2, max: 102, avg: 12.75. Duration: 7.97 ms\u001b[0m\n",
      "\u001b[2mRolling window complete for generic_user in 15.89 ms. Input: 126 rows from 2022-08-04 00:47:51.564611 to 2022-08-04 23:50:26.072379. Output: 421 rows from 2022-08-01 00:03:56.207532 to 2022-08-04 23:50:26.072379\u001b[0m\n",
      "\u001b[2mRolling window complete for generic_user in 14.89 ms. Input: 107 rows from 2022-08-07 00:00:23.959795 to 2022-08-07 23:56:24.809043. Output: 744 rows from 2022-08-01 00:03:56.207532 to 2022-08-07 23:56:24.809043\u001b[0m\n",
      "\u001b[2mPreprocessed 421 data for logs in 2022-08-01 00:03:56.207532 to 2022-08-04 23:50:26.072379 in 1025.0072479248047 ms\u001b[0m\n",
      "\u001b[2mTraining AE model for user: 'generic_user'...\u001b[0m\n",
      "\u001b[2mPreprocessed 744 data for logs in 2022-08-01 00:03:56.207532 to 2022-08-07 23:56:24.809043 in 751.6865730285645 ms\u001b[0m\n",
      "\u001b[2mTraining AE model for user: 'generic_user'... Complete.\u001b[0m\n",
      "\u001b[2mTraining AE model for user: 'generic_user'...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/25 21:03:34 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: DFP-azure-generic_user, version 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mML Flow model upload complete: generic_user:DFP-azure-generic_user:32\u001b[0m\n",
      "\u001b[2mTraining AE model for user: 'generic_user'... Complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/25 21:03:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: DFP-azure-generic_user, version 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mML Flow model upload complete: generic_user:DFP-azure-generic_user:33\u001b[0m\n",
      "====Pipeline Complete====\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create a linear pipeline object\n",
    "configure_logging(log_level=logging.DEBUG)\n",
    "\n",
    "pipeline = LinearPipeline(config)\n",
    "\n",
    "# Source stage\n",
    "pipeline.set_source(MultiFileSource(config, filenames=input_files))\n",
    "\n",
    "# Batch files into batches by time. Use the default ISO date extractor from the filename\n",
    "pipeline.add_stage(\n",
    "    DFPFileBatcherStage(config,\n",
    "                        period=\"D\",\n",
    "                        date_conversion_func=functools.partial(date_extractor, filename_regex=iso_date_regex)))\n",
    "\n",
    "# Output is a list of fsspec files. Convert to DataFrames. This caches downloaded data\n",
    "pipeline.add_stage(\n",
    "    DFPFileToDataFrameStage(config,\n",
    "                            schema=source_schema,\n",
    "                            file_type=FileTypes.JSON,\n",
    "                            parser_kwargs={\n",
    "                                \"lines\": False, \"orient\": \"records\"\n",
    "                            },\n",
    "                            cache_dir=cache_dir))\n",
    "\n",
    "\n",
    "# This will split users or just use one single user\n",
    "pipeline.add_stage(\n",
    "    DFPSplitUsersStage(config,\n",
    "                        include_generic=include_generic,\n",
    "                        include_individual=include_individual,\n",
    "                        skip_users=skip_users))\n",
    "\n",
    "# Next, have a stage that will create rolling windows\n",
    "pipeline.add_stage(\n",
    "    DFPRollingWindowStage(\n",
    "        config,\n",
    "        min_history=300 if is_training else 1,\n",
    "        min_increment=300 if is_training else 0,\n",
    "        # For inference, we only ever want 1 day max\n",
    "        max_history=\"60d\" if is_training else \"1d\",\n",
    "        cache_dir=cache_dir))\n",
    "\n",
    "# Output is UserMessageMeta -- Cached frame set\n",
    "pipeline.add_stage(DFPPreprocessingStage(config, input_schema=preprocess_schema))\n",
    "\n",
    "# Finally, perform training which will output a model\n",
    "pipeline.add_stage(DFPTraining(config, validation_size=0.10))\n",
    "\n",
    "# Write that model to MLFlow\n",
    "pipeline.add_stage(\n",
    "    DFPMLFlowModelWriterStage(config,\n",
    "                                model_name_formatter=model_name_formatter,\n",
    "                                experiment_name_formatter=experiment_name_formatter))\n",
    "\n",
    "# Run the pipeline\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407eebd-23cc-4ac8-8ce3-0c56d589009c",
   "metadata": {},
   "source": [
    "#### That's all! Thanks for reading. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
